{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoConfig,\n",
    ")\n",
    "from IPython.display import Markdown\n",
    "from huggingface_hub import notebook_login\n",
    "from datasets import load_dataset\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "import chromadb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:12: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:12: SyntaxWarning: invalid escape sequence '\\.'\n",
      "/var/folders/85/n4szm2s17gq0mb_1nh56kw9c0000gn/T/ipykernel_52939/104688228.py:12: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  chunk_size=1000, chunk_overlap=60, separators=[\"\\n\\n\", \"\\n\", \"\\.\", \" \", \"\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of splits: 5293\n"
     ]
    }
   ],
   "source": [
    "# Load PTB dataset (Penn Treebank)\n",
    "dataset = load_dataset(\"ptb_text_only\")\n",
    "\n",
    "documents = []\n",
    "for item in dataset[\"train\"]:\n",
    "    # For the \"ptb_text_only\" config, the text is typically in item[\"sentence\"]\n",
    "    text = item[\"sentence\"]\n",
    "    documents.append(text)\n",
    "\n",
    "# Use the recursive character splitter\n",
    "recur_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=60, separators=[\"\\n\\n\", \"\\n\", \"\\.\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Perform the splits using the splitter\n",
    "data_splits = recur_splitter.split_text(\"\".join(documents))\n",
    "print(\"Number of splits:\", len(data_splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/85/n4szm2s17gq0mb_1nh56kw9c0000gn/T/ipykernel_52939/3328698620.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  hf_embeddings = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "### Using embeddings by MPNET: https://huggingface.co/sentence-transformers/all-mpnet-base-v2\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": False}\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = \"./vector_store/\"\n",
    "original_client = chromadb.PersistentClient(path=persist_directory)\n",
    "org_collection = original_client.get_collection(name=\"langchain\")\n",
    "\n",
    "org_data = original_data = org_collection.get(include=[\"embeddings\", \"documents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5293, 768])\n"
     ]
    }
   ],
   "source": [
    "org_emb = torch.Tensor(original_data[\"embeddings\"])\n",
    "print(org_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   0,    3,    2,    1, 5082,   59, 4967, 3340,  496,  387])\n"
     ]
    }
   ],
   "source": [
    "def topk_cosine_sim(x: torch.Tensor, k: int):\n",
    "    x = torch.nn.functional.normalize(x, p=2, dim=1)  # normalize to unit vectors\n",
    "    sim_matrix = x @ x.T  # cosine similarity\n",
    "    topk_sim, topk_idx = torch.topk(\n",
    "        sim_matrix, k=k, dim=1\n",
    "    )  # +1 because self-similarity is 1\n",
    "    return topk_sim[:, :], topk_idx[:, :]  # remove self-match\n",
    "\n",
    "_, brute_force_idx = topk_cosine_sim(org_emb, k=10)\n",
    "print(brute_force_idx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_quantization(tensor: torch.Tensor, clip_val: torch.Tensor, bit):\n",
    "    scale = (2 ** (bit - 1)) - 1\n",
    "    tensor_q: torch.Tensor = tensor.clamp(-clip_val, clip_val) / clip_val * scale\n",
    "    tensor_q = (tensor_q.round() - tensor_q).detach() + tensor_q  # STE 적용\n",
    "    tensor_q_int = tensor_q.to(torch.int8)\n",
    "    # print(tensor_q_int)\n",
    "    msb_2_bits = tensor_q_int & 0xC0\n",
    "    mid_2_bits = tensor_q_int & 0x30\n",
    "    mid2_2_bits = tensor_q_int & 0x0C\n",
    "    lsb_4_bits = tensor_q_int & 0x03\n",
    "    # print(msb_2_bits, mid_2_bits, mid2_2_bits, lsb_4_bits)\n",
    "    msb_2_bits_scaled = msb_2_bits / scale * clip_val\n",
    "    mid_2_bits_scaled = mid_2_bits / scale * clip_val\n",
    "    mid2_2_bits_scaled = mid2_2_bits / scale * clip_val\n",
    "    lsb_4_bits_scaled = lsb_4_bits / scale * clip_val\n",
    "    # print(msb_2_bits_scaled, mid_2_bits_scaled, mid2_2_bits_scaled, lsb_4_bits_scaled)\n",
    "    # return msb_2_bits_scaled, mid_2_bits_scaled, mid2_2_bits_scaled, lsb_4_bits_scaled\n",
    "    return tensor_q_int / scale * clip_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_inject_tensor(weight_tensor: torch.Tensor, std: torch.Tensor, typ: bool):\n",
    "    device = weight_tensor.device\n",
    "    std = std.to(device)\n",
    "    if typ:\n",
    "        std_reshaped = std.view(-1, 1) if std.dim() == 1 else std\n",
    "        adjusted_noise = 1.0 + std_reshaped * torch.randn_like(weight_tensor)\n",
    "    else:\n",
    "        adjusted_noise = 1.0 + std * torch.randn_like(weight_tensor)\n",
    "    return torch.mul(weight_tensor, adjusted_noise).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1083)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "cliff_val = org_emb.std() * 3\n",
    "print(cliff_val)\n",
    "org_emb_q_4bit = uniform_quantization(org_emb, cliff_val, 4)\n",
    "\n",
    "q_collection = original_client.get_or_create_collection(\n",
    "    name=f\"4_bit_q_cliff_{cliff_val}\",\n",
    "    # embedding_function=hf_embeddings,\n",
    "    metadata={\"hnsw:space\": \"cosine\"},\n",
    ")\n",
    "quantized_data = q_collection.get(include=[\"embeddings\"])\n",
    "print(len(quantized_data[\"embeddings\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_collection.add(\n",
    "    ids=[str(i) for i in range(len(org_emb_q_4bit))],\n",
    "    embeddings=org_emb_q_4bit.numpy(),\n",
    "    documents=org_data[\"documents\"],\n",
    "),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_data[\"ids\"]\n",
    "id_idx_map = {id: idx for idx, id in enumerate(org_data[\"ids\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: 0.9990553561307387\n",
      "q_recall: 0.954146986586057\n"
     ]
    }
   ],
   "source": [
    "q_match = 0\n",
    "match = 0\n",
    "k = 10\n",
    "\n",
    "for idx, query in enumerate(org_emb):\n",
    "    q_query_result = q_collection.query(\n",
    "        query_embeddings=query.numpy(),\n",
    "        n_results=k,\n",
    "        include=[\"documents\", \"embeddings\"],\n",
    "    )\n",
    "    query_result = org_collection.query(\n",
    "        query_embeddings=query.numpy(),\n",
    "        n_results=k,\n",
    "        include=[\"documents\", \"embeddings\"],\n",
    "    )\n",
    "    query_idx = set(map(lambda x: id_idx_map[x], query_result[\"ids\"][0]))\n",
    "    q_query_idx = set(map(lambda x: int(x), q_query_result[\"ids\"][0]))\n",
    "    bf = set(map(lambda x: int(x), brute_force_idx[idx]))\n",
    "    # print(query_idx, q_query_idx, bf)\n",
    "    match += len(query_idx.intersection(bf))\n",
    "    q_match += len(q_query_idx.intersection(bf))\n",
    "    # break\n",
    "print(\"recall:\", match / (k * len(org_emb)))\n",
    "print(\"q_recall:\", q_match / (k * len(org_emb)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ucsd-vector-db-F8tpV0eq-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
