{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoConfig,\n",
    ")\n",
    "from IPython.display import Markdown\n",
    "from huggingface_hub import notebook_login\n",
    "from datasets import load_dataset\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "import chromadb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:12: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:12: SyntaxWarning: invalid escape sequence '\\.'\n",
      "/var/folders/85/n4szm2s17gq0mb_1nh56kw9c0000gn/T/ipykernel_30227/104688228.py:12: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  chunk_size=1000, chunk_overlap=60, separators=[\"\\n\\n\", \"\\n\", \"\\.\", \" \", \"\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of splits: 5293\n"
     ]
    }
   ],
   "source": [
    "# Load PTB dataset (Penn Treebank)\n",
    "dataset = load_dataset(\"ptb_text_only\")\n",
    "\n",
    "documents = []\n",
    "for item in dataset[\"train\"]:\n",
    "    # For the \"ptb_text_only\" config, the text is typically in item[\"sentence\"]\n",
    "    text = item[\"sentence\"]\n",
    "    documents.append(text)\n",
    "\n",
    "# Use the recursive character splitter\n",
    "recur_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=60, separators=[\"\\n\\n\", \"\\n\", \"\\.\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Perform the splits using the splitter\n",
    "data_splits = recur_splitter.split_text(\"\".join(documents))\n",
    "print(\"Number of splits:\", len(data_splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153f5badf9ab4b06b16b3648b764b3a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk.\n",
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "# Set the model id to load the model from HuggingFace\n",
    "model_id = \"meta-llama/Llama-3.2-3B\"  # context length of 262k\n",
    "# While waiting access to Llama model, you can use the falcon model to run the code.\n",
    "# model_id = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "# Load the default tokenizer for the selected model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Load the model from HuggingFace\n",
    "llama3 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Wrap the model and tokenizer into a text generation pipeline\n",
    "hf_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=llama3,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=50,\n",
    "    min_new_tokens=30,\n",
    "    temperature=0.1,\n",
    "    repetition_penalty=1.2,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages loaded: 77 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Dataset\n",
       "Num. of\n",
       "Comparisons\n",
       "Avg. # Turns\n",
       "per Dialogue\n",
       "Avg. # Tokens\n",
       "per Example\n",
       "Avg. # Tokens\n",
       "in Prompt\n",
       "Avg. # Tokens\n",
       "in Response\n",
       "Anthropic Helpful\n",
       "122,387\n",
       "3.0\n",
       "251.5\n",
       "17.7\n",
       "88.4\n",
       "Anthropic Harmless\n",
       "43,966\n",
       "3.0\n",
       "152.5\n",
       "15.7\n",
       "46.4\n",
       "OpenAI Summarize\n",
       "176,625\n",
       "1.0\n",
       "371.1\n",
       "336.0\n",
       "35.1\n",
       "OpenAI WebGPT\n",
       "13,333\n",
       "1.0\n",
       "237.2\n",
       "48.3\n",
       "188.9\n",
       "StackExchange\n",
       "1,038,480\n",
       "1.0\n",
       "440.2\n",
       "200.1\n",
       "240.2\n",
       "Stanford SHP\n",
       "74,882\n",
       "1.0\n",
       "338.3\n",
       "199.5\n",
       "138.8\n",
       "Synthetic GPT-J\n",
       "33,139\n",
       "1.0\n",
       "123.3\n",
       "13.0\n",
       "110.3\n",
       "Meta (Safety & Helpfulness)\n",
       "1,418,091\n",
       "3.9\n",
       "798.5\n",
       "31.4\n",
       "234.1\n",
       "Total\n",
       "2,919,326\n",
       "1.6\n",
       "595.7\n",
       "108.2\n",
       "216.9\n",
       "Table 6: Statistics of human preference data for reward modeling. We list both the open-source and\n",
       "internally collected human preference data used for reward modeling. Note that a binary human preference\n",
       "comparison contains 2 responses (chosen and rejected) sharing the same prompt (and previous dialogue).\n",
       "Each example consists of a prompt (including previous dialogue if available) and a response, which is the\n",
       "input of the reward model. We report the number of comparisons, the average number of turns per dialogue,\n",
       "the average number of tokens per example, per prompt and per response. More details on Meta helpfulness\n",
       "and safety data per batch can be found in Appendix A.3.1.\n",
       "knows. This prevents cases where, for instance, the two models would have an information mismatch, which\n",
       "could result in favoring hallucinations. The model architecture and hyper-parameters are identical to those\n",
       "of the pretrained language models, except that the classification head for next-token prediction is replaced\n",
       "with a regression head for outputting a scalar reward.\n",
       "Training Objectives.\n",
       "To train the reward model, we convert our collected pairwise human preference data\n",
       "into a binary ranking label format (i.e., chosen & rejected) and enforce the chosen response to have a higher\n",
       "score than its counterpart. We used a binary ranking loss consistent with Ouyang et al. (2022):\n",
       "Lranking = −log(σ(rθ(x, yc) −rθ(x, yr)))\n",
       "(1)\n",
       "where rθ(x, y) is the scalar score output for prompt x and completion y with model weights θ. yc is the\n",
       "preferred response that annotators choose and yr is the rejected counterpart.\n",
       "Built on top of this binary ranking loss, we further modify it separately for better helpfulness and safety\n",
       "reward models as follows. Given that our preference ratings is decomposed as a scale of four points (e.g.,\n",
       "significantly better), as presented in Section 3.2.1, it can be useful to leverage this information to explicitly\n",
       "teach the reward model to assign more discrepant scores to the generations that have more differences. To\n",
       "do so, we further add a margin component in the loss:\n",
       "Lranking = −log(σ(rθ(x, yc) −rθ(x, yr) −m(r)))\n",
       "(2)\n",
       "where the margin m(r) is a discrete function of the preference rating. Naturally, we use a large margin\n",
       "for pairs with distinct responses, and a smaller one for those with similar responses (shown in Table 27).\n",
       "We found this margin component can improve Helpfulness reward model accuracy especially on samples\n",
       "where two responses are more separable. More detailed ablation and analysis can be found in Table 28 in\n",
       "Appendix A.3.3.\n",
       "Data Composition.\n",
       "We combine our newly collected data with existing open-source preference datasets\n",
       "to form a larger training dataset. Initially, open-source datasets were used to bootstrap our reward models\n",
       "while we were in the process of collecting preference annotation data. We note that in the context of RLHF in\n",
       "this study, the role of reward signals is to learn human preference for Llama 2-Chat outputs rather than\n",
       "any model outputs. However, in our experiments, we do not observe negative transfer from the open-source\n",
       "preference datasets. Thus, we have decided to keep them in our data mixture, as they could enable better\n",
       "generalization for the reward model and prevent reward hacking, i.e. Llama 2-Chat taking advantage of\n",
       "some weaknesses of our reward, and so artificially inflating the score despite performing less well.\n",
       "With training data available from different sources, we experimented with different mixing recipes for both\n",
       "Helpfulness and Safety reward models to ascertain the best settings. After extensive experimentation, the\n",
       "11"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the PDF loader\n",
    "pdf_loader = PyMuPDFLoader(\"llama2.pdf\")\n",
    "\n",
    "# Load data from the pdf\n",
    "pages = pdf_loader.load()\n",
    "\n",
    "# Observe number of pages loaded\n",
    "print(\"Number of pages loaded: {} \\n\".format(len(pages)))\n",
    "\n",
    "Markdown(pages[10].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/85/n4szm2s17gq0mb_1nh56kw9c0000gn/T/ipykernel_30227/3328698620.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  hf_embeddings = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "### Using embeddings by MPNET: https://huggingface.co/sentence-transformers/all-mpnet-base-v2\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": False}\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for the first time if you don't have the original vector db\n",
    "# Define the location to persist data\n",
    "persist_directory = \"./vector_store/\"\n",
    "# Generate and store embeddings\n",
    "\n",
    "# vectordb = Chroma.from_texts(\n",
    "#     texts=data_splits,\n",
    "#     embedding=hf_embeddings,\n",
    "#     persist_directory=persist_directory,\n",
    "# )\n",
    "quantized_directory = \"./quantized_vector_store/\"\n",
    "# quantized_vectordb = Chroma(\n",
    "#     collection_name=\"quantized\",\n",
    "#     persist_directory=quantized_directory,\n",
    "#     embedding_function=hf_embeddings,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5293\n"
     ]
    }
   ],
   "source": [
    "original_client = chromadb.PersistentClient(path=\"./vector_store\")\n",
    "org_collection = original_client.get_or_create_collection(\n",
    "    name=\"langchain\", metadata={\"hnsw:space\": \"l2\"}\n",
    ")\n",
    "\n",
    "original_data = org_collection.get(include=[\"embeddings\", \"documents\"])\n",
    "# print(original_data.keys())\n",
    "# print(original_data[\"embeddings\"])\n",
    "original_embeddings = torch.Tensor(original_data[\"embeddings\"])\n",
    "# print(original_embeddings.shape)\n",
    "original_ids = original_data[\"ids\"]\n",
    "original_documents = original_data[\"documents\"]\n",
    "print(len(original_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"the chains include bloomingdale 's owned by campeau corp. toronto saks fifth\"\n",
    "query_embedding = hf_embeddings.embed_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['f780bd65-c9aa-42b8-9f7c-57c4f9cd86f6', '6f13c270-3968-465f-80eb-8bf3c016db6f', '25ff322b-125f-4614-80b6-09479793f47d', '74f447f5-8d53-4101-bd17-0eacd21bacc4']], 'embeddings': [array([[ 0.02809857,  0.052302  ,  0.01488034, ..., -0.04275348,\n",
      "        -0.02404324,  0.0008042 ],\n",
      "       [ 0.05341762, -0.00395871,  0.00827139, ..., -0.04405971,\n",
      "        -0.02083498, -0.01610375],\n",
      "       [ 0.05040516,  0.00120951,  0.01073335, ..., -0.04083301,\n",
      "        -0.04182988, -0.01017789],\n",
      "       [-0.01398617,  0.074091  , -0.02127344, ..., -0.02011109,\n",
      "        -0.02271994, -0.01761337]], shape=(4, 768))], 'documents': [[\"in very early stages stillbloomingdale 's is a <unk> chain acquired last year by campeau in its $ N billion acquisition of federatedbloomingdale 's does an estimated $ N billion in annual salesthe sale of bloomingdale 's is a condition of efforts by toronto-based olympia & york developments ltd. to arrange $ N million in bridge financing for campeau which disclosed last month that its retailing units federated department stores inc. and allied stores corp. were strapped for cash<unk> owned by toronto 's <unk> family is also <unk> major restructuring and refinancing of campeau a toronto-based real estate and retailing companyone executive familiar with the bloomingdale 's situation said no book has been issued regarding bloomingdale 's there are no projections so i doubt very much whether any bid has been madeseparately a campeau shareholder filed suit charging campeau chairman robert campeau and other officers with violating securities lawthe suit filed in u.s. district court in\", \"representing bloomingdale 's chairman marvin traub and more than half are seeking additional information on the group bankers saywhat mr. traub is hoping to put together investment bankers say is a management-led group to buy the new york department-store group that he heads from campeau 's federated department stores subsidiaryfederated ran into a cash crunch after it was acquired last year by campeau which relied heavily on debt to finance the transactionpaying off that debt put such a squeeze on campeau and its stores that federated decided to sell off the <unk> of its retailing empire including bloomingdale 'shoping to avoid another takeover mr. traub retained blackstone group and drexel burnham lambert inc. to help him find partners for a management-led buy-out<unk> investment bankers say he wants to get backing from a japanese department store and a european department store to forge a global retailing networkwhen you look at the economics traub needs a japanese and a european\", \"the sale 's process said a bloomingdale 's spokesmanwe wo n't comment on themtokyu executives were n't available for comment early thursday morning in tokyocampeau 's chairman robert campeau said at its annual meeting in july that he valued bloomingdale 's at $ N billionamong previously disclosed possible bidders is bloomingdale 's chairman marvin traub who has aligned himself with drexel burnham lambert inc. and blackstone groupinvestment bankers in tokyo confirmed that tokyu department store is one of several japanese companies that has been approached by representatives of a management committee headed by bloomingdale 's mr. traubbut they said detailed financial figures have n't been passed yet to any prospective buyersnobody is going to make a real bid before the middle of november said one investment banker familiar with the discussions in japantokyu is one of the potential buyers who might raise its handbut it 's in very early stages stillbloomingdale 's is a <unk> chain\", \"code b.a.t industries plc 's healthy saks fifth avenue and marshall field 's chains are on the auction block campeau 's bloomingdale 's is also on the blockindustry observers expect a wide divergence in performancestores in a state of confusion are likely to fare poorly and to lose customers to stable chains such as limited inc. may department stores co. and <unk> department stores inc. which should do wellthere are going to be very clear winners and very clear losers says cynthia <unk> a <unk> ross & co. retail consultantsays mr. <unk> i 'm looking for a <unk> christmaseconomists expect general merchandise sales in the fourth quarter to rise N N to N N from year-ago figuresbut mr. <unk> predicts that healthy stores <unk> mostly apparel could ring up gains of as much as N N to N Ntroubled chains could see their sales drop as much as N N he believes as managers <unk> by fears about the future allow their stores to get sloppythin merchandise <unk> at the most troubled chains are also\"]], 'uris': None, 'data': None, 'metadatas': None, 'distances': None, 'included': [<IncludeEnum.embeddings: 'embeddings'>, <IncludeEnum.documents: 'documents'>]}\n"
     ]
    }
   ],
   "source": [
    "org_result = org_collection.query(\n",
    "    query_embedding,\n",
    "    n_results=4,\n",
    "    include=[\"embeddings\", \"documents\"],\n",
    ")\n",
    "print(org_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0 vectors from 'quantized_vectordb'.\n"
     ]
    }
   ],
   "source": [
    "# Quantize the embeddings\n",
    "quantized_client = chromadb.PersistentClient(path=\"./quantized_vector_store\")\n",
    "quantized_collection = quantized_client.get_or_create_collection(\n",
    "    name=\"quantized\", metadata={\"hnsw:space\": \"l2\"}\n",
    ")\n",
    "quantized_data = quantized_collection.get(include=[\"embeddings\"])\n",
    "quantized_embeddings = torch.Tensor(quantized_data[\"embeddings\"])\n",
    "print(f\"Loaded {len(quantized_embeddings)} vectors from 'quantized_vectordb'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_collection.delete(ids=original_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_quantization(tensor: torch.Tensor, clip_val: torch.Tensor, bit):\n",
    "    scale = (2 ** (bit - 1)) - 1\n",
    "    tensor_q: torch.Tensor = tensor.clamp(-clip_val, clip_val) / clip_val * scale\n",
    "    tensor_q = (tensor_q.round() - tensor_q).detach() + tensor_q  # STE 적용\n",
    "    tensor_q_int = tensor_q.to(torch.int8)\n",
    "    # print(tensor_q_int)\n",
    "    msb_2_bits = tensor_q_int & 0xC0\n",
    "    mid_2_bits = tensor_q_int & 0x30\n",
    "    mid2_2_bits = tensor_q_int & 0x0C\n",
    "    lsb_4_bits = tensor_q_int & 0x03\n",
    "    # print(msb_2_bits, mid_2_bits, mid2_2_bits, lsb_4_bits)\n",
    "    msb_2_bits_scaled = msb_2_bits / scale * clip_val\n",
    "    mid_2_bits_scaled = mid_2_bits / scale * clip_val\n",
    "    mid2_2_bits_scaled = mid2_2_bits / scale * clip_val\n",
    "    lsb_4_bits_scaled = lsb_4_bits / scale * clip_val\n",
    "    # print(msb_2_bits_scaled, mid_2_bits_scaled, mid2_2_bits_scaled, lsb_4_bits_scaled)\n",
    "    # return msb_2_bits_scaled, mid_2_bits_scaled, mid2_2_bits_scaled, lsb_4_bits_scaled\n",
    "    return tensor_q_int / scale * clip_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_inject_tensor(weight_tensor: torch.Tensor, std: torch.Tensor, typ: bool):\n",
    "    device = weight_tensor.device\n",
    "    std = std.to(device)\n",
    "    if typ:\n",
    "        std_reshaped = std.view(-1, 1) if std.dim() == 1 else std\n",
    "        adjusted_noise = 1.0 + std_reshaped * torch.randn_like(weight_tensor)\n",
    "    else:\n",
    "        adjusted_noise = 1.0 + std * torch.randn_like(weight_tensor)\n",
    "    return torch.mul(weight_tensor, adjusted_noise).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_collection.add(\n",
    "    embeddings=uniform_quantization(original_embeddings, 1.0, 8).numpy(),\n",
    "    ids=original_ids,\n",
    "    documents=original_documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0 vectors from 'quantized_noise_vectordb'.\n"
     ]
    }
   ],
   "source": [
    "# Quantize the embeddings\n",
    "qn_client = chromadb.PersistentClient(path=\"./quantized_noise_vector_store\")\n",
    "qn_collection = quantized_client.get_or_create_collection(\n",
    "    name=\"quantized_noise\", metadata={\"hnsw:space\": \"l2\"}\n",
    ")\n",
    "qn_data = qn_collection.get(include=[\"embeddings\"])\n",
    "qn_embeddings = torch.Tensor(qn_data[\"embeddings\"])\n",
    "print(f\"Loaded {len(qn_embeddings)} vectors from 'quantized_noise_vectordb'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "qn_collection.add(\n",
    "    ids=original_ids,\n",
    "    documents=original_documents,\n",
    "    embeddings=noise_inject_tensor(\n",
    "        weight_tensor=uniform_quantization(\n",
    "            tensor=original_embeddings,\n",
    "            clip_val=1.0,\n",
    "            bit=8,\n",
    "        ),\n",
    "        std=torch.Tensor([0.03]),\n",
    "        typ=True,\n",
    "    ).numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/85/n4szm2s17gq0mb_1nh56kw9c0000gn/T/ipykernel_71464/623732966.py:1: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  original_db: Chroma = Chroma(\n",
      "/var/folders/85/n4szm2s17gq0mb_1nh56kw9c0000gn/T/ipykernel_71464/623732966.py:6: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  original_db.persist()\n"
     ]
    }
   ],
   "source": [
    "original_db: Chroma = Chroma(\n",
    "    collection_name=\"langchain\",\n",
    "    persist_directory=\"./vector_store\",\n",
    "    embedding_function=hf_embeddings,\n",
    ")\n",
    "original_db.persist()\n",
    "quantized_db: Chroma = Chroma(\n",
    "    collection_name=\"quantized\",\n",
    "    persist_directory=\"./quantized_vector_store\",\n",
    "    embedding_function=hf_embeddings,\n",
    ")\n",
    "quantized_db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"the chains include bloomingdale 's owned by campeau corp. toronto saks fifth\"\n",
    "query_embedding = hf_embeddings.embed_query(query)\n",
    "q_query_embedding = uniform_quantization(torch.Tensor(query_embedding), 1.0, 8).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['ids', 'embeddings', 'documents', 'uris', 'data', 'metadatas', 'distances', 'included'])\n",
      "['f780bd65-c9aa-42b8-9f7c-57c4f9cd86f6', '6f13c270-3968-465f-80eb-8bf3c016db6f', '25ff322b-125f-4614-80b6-09479793f47d', '74f447f5-8d53-4101-bd17-0eacd21bacc4']\n",
      "dict_keys(['ids', 'embeddings', 'documents', 'uris', 'data', 'metadatas', 'distances', 'included'])\n",
      "['f780bd65-c9aa-42b8-9f7c-57c4f9cd86f6', '6f13c270-3968-465f-80eb-8bf3c016db6f', '25ff322b-125f-4614-80b6-09479793f47d', '74f447f5-8d53-4101-bd17-0eacd21bacc4']\n",
      "tensor(0.0634) tensor(0.0710)\n",
      "tensor(0.0639) tensor(0.0695)\n",
      "tensor(0.0628) tensor(0.0688)\n",
      "tensor(0.0622) tensor(0.0692)\n"
     ]
    }
   ],
   "source": [
    "org_result = org_collection.query(\n",
    "    query_embeddings=query_embedding,\n",
    "    n_results=4,\n",
    "    include=[\"embeddings\", \"documents\", \"distances\"],\n",
    ")\n",
    "print(org_result.keys())\n",
    "query_ids = org_result[\"ids\"][0]\n",
    "print(query_ids)\n",
    "# query_docs = org_result[\"documents\"][0]\n",
    "# print(query_docs)\n",
    "# query_dist = org_result[\"distances\"][0]\n",
    "# print(query_dist)\n",
    "# query_emb = org_result[\"embeddings\"]\n",
    "q_result = quantized_collection.query(\n",
    "    query_embeddings=q_query_embedding,\n",
    "    n_results=4,\n",
    "    include=[\"embeddings\", \"documents\", \"distances\"],\n",
    ")\n",
    "print(q_result.keys())\n",
    "q_query_ids = q_result[\"ids\"][0]\n",
    "# q_query_embs = q_result[\"embeddings\"]\n",
    "# print(q_query_embs)\n",
    "# print(q_query_ids)\n",
    "\n",
    "\n",
    "qn_result = qn_collection.query(\n",
    "    query_embeddings=q_query_embedding,\n",
    "    n_results=4,\n",
    "    include=[\"embeddings\", \"documents\", \"distances\"],\n",
    ")\n",
    "print(qn_result[\"ids\"][0])\n",
    "for i in range(4):\n",
    "    org_emb = org_result[\"embeddings\"][0][i]\n",
    "    q_emb = q_result[\"embeddings\"][0][i]\n",
    "    qn_emb = qn_result[\"embeddings\"][0][i]\n",
    "    print(\n",
    "        torch.norm(torch.Tensor(org_emb) - torch.Tensor(q_emb)),\n",
    "        torch.norm(torch.Tensor(org_emb) - torch.Tensor(qn_emb)),\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ucsd-vector-db-F8tpV0eq-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
